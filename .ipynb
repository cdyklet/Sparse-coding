{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Optimizer\n",
    "gpu = tf.config.experimental.get_visible_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(gpu, enable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamCustom(Optimizer):\n",
    "    def __init__(self, learning_rate = 0.001, beta1= 0.9, beta2= 0.999, use_locking = False, \n",
    "                 name = 'Adam_Custom', decay = False, **kwargs):\n",
    "        super(AdamCustom, self).__init__(name, **kwargs)\n",
    "        self._name = name\n",
    "        self._lr = learning_rate\n",
    "        \n",
    "        self._beta1 = tf.constant(beta1, dtype = tf.float32)\n",
    "        self._beta2 = tf.constant(beta2, dtype = tf.float32)\n",
    "        # tensor versions of the constructor arguments, created in _prepare\n",
    "        self._lr_t = None\n",
    "        self._beta1_t = tf.Variable(0, dtype = tf.float32, trainable = False)\n",
    "        self._beta2_t = tf.Variable(0, dtype = tf.float32, trainable = False)\n",
    "        self._decay = decay\n",
    "    def _create_slots(self, var_list):\n",
    "        # create slots for the first and second moments\n",
    "        for v in var_list:\n",
    "            self.add_slot(v, slot_name = 'v', initializer = 'zeros')\n",
    "            self.add_slot(v, slot_name = 'm', initializer = 'zeros')\n",
    "    def _resource_apply_dense(self, grad, var, **kw_args):\n",
    "        var_device, var_dtype = var.device,var.dtype.base_dtype\n",
    "        lr_t = tf.cast(self._lr, var_dtype)\n",
    "        beta1 = tf.cast(self._beta1, var_dtype)\n",
    "        beta2 = tf.cast(self._beta2, var_dtype)\n",
    "        beta1_t = tf.cast(self._beta1_t, var_dtype)\n",
    "        beta2_t = tf.cast(self._beta2_t, var_dtype)\n",
    "        eps = 1e-7\n",
    "        m = self.get_slot(var, 'm')\n",
    "        v = self.get_slot(var, 'v')\n",
    "        m_t = m.assign(beta2 * m + (1- beta2) * grad**2)\n",
    "        v_t = v.assign(beta1 * v + (1 -beta1) * grad)\n",
    "        m_refined = m_t /(1 - beta2_t)\n",
    "        v_refined = v_t /(1 - beta1_t)\n",
    "        # decay learning rate\n",
    "        if self._decay:\n",
    "            step = tf.cast(self.iterations, dtype = tf.float32)\n",
    "            lr_t = lr_t/tf.sqrt(step+1)\n",
    "        var_delta = lr_t * v_refined / (tf.sqrt(m_refined) + eps)\n",
    "        var_update = var.assign_sub(var_delta)\n",
    "        #print(var_delta.numpy(), m_t.numpy(), v_t.numpy(), m.numpy(), v.numpy())\n",
    "        self._beta1_t.assign(self._beta1_t * self._beta1)\n",
    "        self._beta2_t.assign(self._beta2_t * self._beta2)\n",
    "        return var_update\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError('Sparse gradient updates are not supported')   \n",
    "    def get_config(self, ):\n",
    "        config = {'name': self._name}\n",
    "        if self.clipnorm is not None:\n",
    "            config['clipnorm'] = self.clipnorm\n",
    "        if self.clipvalur is not None:\n",
    "            config['clipvalue'] = self.clipvalue\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMSCustom(Optimizer):\n",
    "    def __init__(self, learning_rate = 0.001, beta1= 0.9, beta2= 0.999, use_locking = False, \n",
    "                 name = 'Adam_Custom', decay = False, **kwargs):\n",
    "        super(AMSCustom, self).__init__(name, **kwargs)\n",
    "        self._name = name\n",
    "        self._lr = learning_rate\n",
    "        \n",
    "        self._beta1 = tf.constant(beta1, dtype = tf.float32)\n",
    "        self._beta2 = tf.constant(beta2, dtype = tf.float32)\n",
    "        # tensor versions of the constructor arguments, created in _prepare\n",
    "        self._lr_t = None\n",
    "        self._beta1_t = tf.Variable(0, dtype = tf.float32, trainable = False)\n",
    "        self._beta2_t = tf.Variable(0, dtype = tf.float32, trainable = False)\n",
    "        self._decay = decay\n",
    "    def _create_slots(self, var_list):\n",
    "        # create slots for the first and second moments\n",
    "        for v in var_list:\n",
    "            self.add_slot(v, slot_name = 'v', initializer = 'zeros')\n",
    "            self.add_slot(v, slot_name = 'm', initializer = 'zeros')\n",
    "            self.add_slot(v, slot_name = 'v_hat')\n",
    "    def _resource_apply_dense(self, grad, var, **kw_args):\n",
    "        var_device, var_dtype = var.device,var.dtype.base_dtype\n",
    "        lr_t = tf.cast(self._lr, var_dtype)\n",
    "        if self._decay:\n",
    "            step = tf.cast(self.iterations, dtype = tf.float32)\n",
    "            lr_t = lr_t/tf.sqrt(step+1)\n",
    "        beta1 = tf.cast(self._beta1, var_dtype)\n",
    "        beta2 = tf.cast(self._beta2, var_dtype)\n",
    "        beta1_t = tf.cast(self._beta1_t, var_dtype)\n",
    "        beta2_t = tf.cast(self._beta2_t, var_dtype)\n",
    "        eps = 1e-7\n",
    "        m = self.get_slot(var, 'm')\n",
    "        v = self.get_slot(var, 'v')\n",
    "        v_hat = self.get_slot(var, 'v_hat')\n",
    "        m_t = m.assign(beta2 * m + (1- beta2) * grad**2)\n",
    "        v_t = v.assign(beta1 * v + (1 -beta1) * grad)\n",
    "        m_refined = m_t /(1 - beta2_t)\n",
    "        v_refined = v_t /(1 - beta1_t)\n",
    "        # decay learning rate\n",
    "        m_refined = v_hat.assign(tf.maximum(m_refined, v_hat))\n",
    "        var_delta = lr_t * v_refined / (tf.sqrt(m_refined) + eps)\n",
    "        var_update = var.assign_sub(var_delta)\n",
    "        #print(var_delta.numpy(), m_t.numpy(), v_t.numpy(), m.numpy(), v.numpy())\n",
    "        self._beta1_t.assign(self._beta1_t * self._beta1)\n",
    "        self._beta2_t.assign(self._beta2_t * self._beta2)\n",
    "        return var_update\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError('Sparse gradient updates are not supported')   \n",
    "    def get_config(self, ):\n",
    "        config = {'name': self._name}\n",
    "        if self.clipnorm is not None:\n",
    "            config['clipnorm'] = self.clipnorm\n",
    "        if self.clipvalur is not None:\n",
    "            config['clipvalue'] = self.clipvalue\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = tfp.distributions.Bernoulli(probs = 0.01, dtype = tf.float32)\n",
    "def test_optimizer(optimizer_name, iterations = 500000, learning_rate = 0.0001,decay = True):\n",
    "    x = tf.Variable(0, dtype = tf.float32)\n",
    "    r = bernoulli.sample()\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = AdamCustom(learning_rate = 0.001, decay = decay)\n",
    "    elif optimizer_name == 'ams':\n",
    "        optimizer = AMSCustom(learning_rate = 0.001, decay = decay)\n",
    "    results = []\n",
    "    quan = iterations //10\n",
    "    for i in range(iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x)\n",
    "            loss = (1010 *r -10*(1-r))*x\n",
    "        gradient = tape.gradient(loss, x)\n",
    "        optimizer.apply_gradients([(gradient, x)])\n",
    "        if  i % quan ==0:\n",
    "            print('current x,:{}, loss:{}'.format(x.numpy(), loss.numpy()))\n",
    "        results.append(x.numpy())\n",
    "        x.assign(tf.clip_by_value(x, -1, 1))\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current x,:0.0031622983515262604, loss:-0.0\n",
      "current x,:0.5610617399215698, loss:-5.610572814941406\n",
      "current x,:0.7462995648384094, loss:-7.462964057922363\n",
      "current x,:0.8884552717208862, loss:-8.884527206420898\n",
      "current x,:1.0000022649765015, loss:-10.0\n",
      "current x,:1.0000020265579224, loss:-10.0\n",
      "current x,:1.0000017881393433, loss:-10.0\n",
      "current x,:1.0000016689300537, loss:-10.0\n",
      "current x,:1.0000015497207642, loss:-10.0\n",
      "current x,:1.0000015497207642, loss:-10.0\n"
     ]
    }
   ],
   "source": [
    "ret1 = test_optimizer('adam',decay = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current x,:0.0031622983515262604, loss:-0.0\n",
      "current x,:0.5610617399215698, loss:-5.610572814941406\n",
      "current x,:0.7462995648384094, loss:-7.462964057922363\n",
      "current x,:0.8884552717208862, loss:-8.884527206420898\n",
      "current x,:1.0000022649765015, loss:-10.0\n",
      "current x,:1.0000020265579224, loss:-10.0\n",
      "current x,:1.0000017881393433, loss:-10.0\n",
      "current x,:1.0000016689300537, loss:-10.0\n",
      "current x,:1.0000015497207642, loss:-10.0\n",
      "current x,:1.0000015497207642, loss:-10.0\n"
     ]
    }
   ],
   "source": [
    "ret2 = test_optimizer('ams',decay = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9910.001 99.68377 9.68377\n",
      "9841.262 99.3833 9.383567\n",
      "9776.325 99.09756 9.09849\n",
      "9714.912 98.825584 8.827688\n",
      "9656.771 98.56649 8.570356\n",
      "9601.672 98.31945 8.325733\n",
      "9549.394 98.083694 8.093101\n",
      "9499.742 97.85851 7.8717804\n",
      "9452.537 97.64325 7.6611323\n",
      "9407.611 97.43729 7.460552\n",
      "9364.81 97.24006 7.2694707\n",
      "9323.993 97.05103 7.087351\n",
      "9285.031 96.86971 6.9136868\n",
      "9247.802 96.69564 6.748002\n",
      "9212.19 96.5284 6.589848\n",
      "9178.101 96.36758 6.438802\n",
      "9145.433 96.21281 6.294465\n",
      "9114.099 96.06375 6.156464\n",
      "9084.019 95.92008 6.024446\n",
      "9055.115 95.7815 5.8980794\n",
      "9027.319 95.64773 5.777053\n",
      "9000.566 95.5185 5.6610723\n",
      "8974.795 95.39357 5.549862\n",
      "8949.946 95.27271 5.4431624\n",
      "8925.973 95.155716 5.3407297\n",
      "8902.822 95.04237 5.242334\n",
      "8880.45 94.9325 5.1477594\n",
      "8858.813 94.82593 5.0568027\n",
      "8837.876 94.72248 4.969273\n",
      "8817.596 94.62201 4.884991\n",
      "8797.943 94.52437 4.8037877\n",
      "8778.884 94.42943 4.7255034\n",
      "8760.389 94.33705 4.649989\n",
      "8742.428 94.24712 4.5771036\n",
      "8724.977 94.15953 4.506715\n",
      "8708.009 94.074165 4.4386973\n",
      "8691.503 93.99093 4.3729343\n",
      "8675.436 93.90973 4.309315\n",
      "8659.788 93.830475 4.2477355\n",
      "8644.54 93.75308 4.188097\n",
      "8629.675 93.677475 4.130307\n",
      "8615.174 93.60358 4.074279\n",
      "8601.022 93.53132 4.01993\n",
      "8587.205 93.46063 3.9671824\n",
      "8573.708 93.39146 3.9159634\n",
      "8560.516 93.32373 3.8662035\n",
      "8547.618 93.25739 3.817838\n",
      "8535.003 93.1924 3.7708046\n",
      "8522.657 93.12869 3.7250457\n",
      "8510.572 93.06622 3.680506\n",
      "8498.735 93.00495 3.6371334\n",
      "8487.14 92.94483 3.5948787\n",
      "8475.775 92.88582 3.5536952\n",
      "8464.633 92.82788 3.513539\n",
      "8453.704 92.77097 3.4743683\n",
      "8442.982 92.715065 3.4361434\n",
      "8432.459 92.660126 3.3988268\n",
      "8422.13 92.60612 3.3623831\n",
      "8411.986 92.55301 3.3267784\n",
      "8402.0205 92.50078 3.2919805\n",
      "8392.2295 92.449394 3.2579594\n",
      "8382.606 92.39883 3.2246857\n",
      "8373.145 92.34905 3.1921322\n",
      "8363.839 92.30004 3.1602728\n",
      "8354.686 92.25178 3.1290827\n",
      "8345.679 92.20424 3.0985382\n",
      "8336.814 92.1574 3.0686166\n",
      "8328.089 92.111244 3.0392969\n",
      "8319.495 92.06575 3.0105581\n",
      "8311.034 92.0209 2.982381\n",
      "8302.698 91.97667 2.9547472\n",
      "8294.485 91.93305 2.9276385\n",
      "8286.392 91.89002 2.9010382\n",
      "8278.412 91.84757 2.8749301\n",
      "8270.547 91.80568 2.8492987\n",
      "8262.79 91.76433 2.824129\n",
      "8255.139 91.72351 2.7994072\n",
      "8247.593 91.68321 2.7751195\n",
      "8240.1455 91.64342 2.7512531\n",
      "8232.799 91.60411 2.7277954\n",
      "8225.545 91.565285 2.7047346\n",
      "8218.387 91.52693 2.682059\n",
      "8211.319 91.48903 2.659758\n",
      "8204.34 91.45158 2.637821\n",
      "8197.445 91.41456 2.6162376\n",
      "8190.6377 91.37797 2.5949984\n",
      "8183.9106 91.34179 2.574094\n",
      "8177.2646 91.306015 2.5535154\n",
      "8170.697 91.27064 2.5332541\n",
      "8164.204 91.23565 2.5133016\n",
      "8157.79 91.20104 2.49365\n",
      "8151.4473 91.1668 2.4742916\n",
      "8145.1733 91.13293 2.455219\n",
      "8138.9717 91.0994 2.4364252\n",
      "8132.839 91.06623 2.4179032\n",
      "8126.7715 91.033394 2.3996463\n",
      "8120.7705 91.00089 2.381648\n",
      "8114.8335 90.96872 2.3639026\n",
      "8108.958 90.93687 2.3464036\n",
      "8103.1455 90.90533 2.3291454\n"
     ]
    }
   ],
   "source": [
    "optimizer = AMSCustom(learning_rate = 0.1, decay = True)\n",
    "x = tf.Variable(100, trainable = True,dtype = tf.float32)\n",
    "y = tf.Variable(10, trainable = True, dtype = tf.float32)\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([x,y])\n",
    "        loss = x**2 - 2*x + 10 + y**2\n",
    "    gradient = tape.gradient(loss, [x,y])\n",
    "    _ = optimizer.apply_gradients(zip(gradient, [x, y]))\n",
    "    print(loss.numpy(), x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9809.992 98.99997 9.00188\n",
      "9612.983 98.00024 8.005105\n",
      "9418.032 97.00098 7.0124235\n",
      "9225.172 96.00237 6.0268927\n",
      "9034.43 95.004616 5.0515857\n",
      "8845.84 94.0079 4.088662\n",
      "8659.432 93.01241 3.1377559\n",
      "8475.234 92.01836 2.1934059\n",
      "8293.273 91.02592 1.2416033\n",
      "8113.5796 90.035286 0.26432884\n",
      "7936.183 89.04666 -0.67231077\n",
      "7761.118 88.06023 -1.3123233\n",
      "7588.3975 87.07617 -1.6103289\n",
      "7418.0273 86.09468 -1.6441551\n",
      "7250.026 85.11594 -1.4788241\n",
      "7084.408 84.14012 -1.1618159\n",
      "6921.192 83.167404 -0.7347004\n",
      "6760.387 82.19797 -0.24562538\n",
      "6602.01 81.23198 0.2383686\n",
      "6446.071 80.2696 0.6370436\n",
      "6292.5737 79.311005 0.89363766\n",
      "6141.5205 78.35634 0.9932953\n",
      "5992.9116 77.40576 0.9478514\n",
      "5846.749 76.45943 0.7807756\n",
      "5703.0312 75.51747 0.52250916\n",
      "5561.756 74.58004 0.21155402\n",
      "5422.923 73.64727 -0.10465741\n",
      "5286.526 72.71929 -0.37578142\n",
      "5152.558 71.796234 -0.5620746\n",
      "5021.01 70.87821 -0.6440052\n",
      "4891.8687 69.96535 -0.6212798\n",
      "4765.123 69.057755 -0.50749856\n",
      "4640.7603 68.15555 -0.32635933\n",
      "4518.768 67.25883 -0.10963862\n",
      "4399.1323 66.36769 0.10578588\n",
      "4281.835 65.48224 0.28418258\n",
      "4166.8604 64.602554 0.39884055\n",
      "4054.1868 63.728733 0.4368928\n",
      "3943.7954 62.86086 0.39938506\n",
      "3835.6672 61.999012 0.2989016\n",
      "3729.7805 61.14327 0.15685324\n",
      "3626.1128 60.2937 0.00039067864\n",
      "3524.6428 59.45038 -0.14202474\n",
      "3425.347 58.613365 -0.24619448\n",
      "3328.2002 57.782726 -0.29689366\n",
      "3233.179 56.95852 -0.28989145\n",
      "3140.2568 56.140804 -0.23140685\n",
      "3049.4092 55.32963 -0.1361228\n",
      "2960.6091 54.52505 -0.024329782\n",
      "2873.831 53.72711 0.08184206\n",
      "2789.0486 52.935856 0.16279398\n",
      "2706.2332 52.15133 0.20529343\n",
      "2625.3586 51.37357 0.20458719\n",
      "2546.397 50.602615 0.16451687\n",
      "2469.3196 49.838497 0.096098766\n",
      "2394.0989 49.081253 0.015031889\n",
      "2320.7068 48.33091 -0.06157151\n",
      "2249.115 47.587494 -0.11873528\n",
      "2179.2947 46.851032 -0.14659683\n",
      "2111.2173 46.12155 -0.14204557\n",
      "2044.8545 45.39907 -0.10879482\n",
      "1980.1775 44.68361 -0.05611668\n",
      "1917.1576 43.975185 0.0034026504\n",
      "1855.7666 43.273815 0.05668276\n",
      "1795.9755 42.57951 0.09297408\n",
      "1737.7557 41.892284 0.10601446\n",
      "1681.0791 41.21215 0.09502967\n",
      "1625.9172 40.539116 0.06446982\n",
      "1572.2417 39.873184 0.02267075\n",
      "1520.0245 39.214363 -0.020222317\n",
      "1469.2377 38.562656 -0.05461861\n",
      "1419.8531 37.918068 -0.07359089\n",
      "1371.8436 37.280598 -0.07423463\n",
      "1325.1818 36.65024 -0.058009766\n",
      "1279.8397 36.027 -0.030077841\n",
      "1235.7908 35.41087 0.0021424629\n",
      "1193.008 34.80184 0.030841688\n",
      "1151.4646 34.199913 0.04966223\n",
      "1111.1343 33.605072 0.055076487\n",
      "1071.9908 33.01731 0.04699591\n",
      "1034.0082 32.43662 0.02852473\n",
      "997.16095 31.86298 0.004981011\n",
      "961.4235 31.296383 -0.01753971\n",
      "926.77075 30.736813 -0.033691682\n",
      "893.17804 30.184252 -0.040118642\n",
      "860.6206 29.638681 -0.03611677\n",
      "829.07416 29.100084 -0.02359851\n",
      "798.5147 28.568438 -0.0064175576\n",
      "768.91876 28.043722 0.010736443\n",
      "740.2629 27.525913 0.023573313\n",
      "712.52405 27.014986 0.029248996\n",
      "685.6796 26.510916 0.026969356\n",
      "659.70685 26.013676 0.018028552\n",
      "634.5841 25.52324 0.0053131236\n",
      "610.2893 25.039576 -0.0075645894\n",
      "586.8012 24.562656 -0.017256618\n",
      "564.09875 24.09245 -0.021527784\n",
      "542.16125 23.62892 -0.019749887\n",
      "520.968 23.172037 -0.012936992\n",
      "500.4992 22.721766 -0.003345809\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(100, trainable = True,dtype = tf.float32)\n",
    "y = tf.Variable(10, trainable = True, dtype = tf.float32)\n",
    "optimizer1 = tf.keras.optimizers.Adam(learning_rate = 1)\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([x,y])\n",
    "        loss = x**2 - 2*x + 10 - 1/(y**2 + 10)\n",
    "    gradient = tape.gradient(loss, [x,y])\n",
    "    _ = optimizer1.apply_gradients(zip(gradient, [x, y]))\n",
    "    print(loss.numpy(), x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "import abc\n",
    "@six.add_metaclass(abc.ABCMeta)\n",
    "class Bass():\n",
    "    @abc.abstractmethod\n",
    "    def whatever(self,):\n",
    "        raise NotImplementedError\n",
    "class SubClass(Bass):\n",
    "    def __init__(self,):\n",
    "        super(SubClass, self).__init__()\n",
    "    def whatever(self,):\n",
    "        print('whatever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9f6bd9ad4d88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhatever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a.whatever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubClass1(Bass):\n",
    "    def __init__(self, ):\n",
    "        super(SubClass1, self).__init__()\n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = SubClass1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b._A__name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python37]",
   "language": "python",
   "name": "conda-env-python37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
